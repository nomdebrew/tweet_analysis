{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nomdebrew/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nomdebrew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pygal\n",
    "from pygal.style import Style\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "data = pd.read_csv(\"csv/tweets.csv\", sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Drop columns except 'created_str' and 'text', separate data, and perform sentiment analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomdebrew/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/nomdebrew/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/home/nomdebrew/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/nomdebrew/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/nomdebrew/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203430, 2)\n",
      "(25555, 4)\n",
      "(40403, 4)\n",
      "(57521, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomdebrew/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "columns_to_use = ['created_str','text']\n",
    "data = data[columns_to_use]\n",
    "data = data.dropna(subset=['text'])\n",
    "\n",
    "#convert created_str to datetime and drop hour, min, sec\n",
    "data['created_str'] = pd.to_datetime(data['created_str'])\n",
    "data.created_str = data.created_str.map(lambda x: x.replace(day=1, hour=0, minute=0, second=0))\n",
    "\n",
    "\n",
    "clintonData = data[data['text'].str.contains('clinton|hillary', case=False)]\n",
    "trumpData = data[data['text'].str.contains('trump|donald', case=False)]\n",
    "bothData = data[data['text'].str.contains('clinton|hillary|trump|donald', case=False)]\n",
    "\n",
    "#add polarity and subjectivity columns to dataframes\n",
    "clintonData['polarity'] = clintonData['text'].apply(lambda tweet: TextBlob(tweet).sentiment.polarity)\n",
    "clintonData['subjectivity'] = clintonData['text'].apply(lambda tweet: TextBlob(tweet).sentiment.subjectivity)\n",
    "trumpData['polarity'] = trumpData['text'].apply(lambda tweet: TextBlob(tweet).sentiment.polarity)\n",
    "trumpData['subjectivity'] = trumpData['text'].apply(lambda tweet: TextBlob(tweet).sentiment.subjectivity)\n",
    "bothData['polarity'] = bothData['text'].apply(lambda tweet: TextBlob(tweet).sentiment.polarity)\n",
    "bothData['subjectivity'] = bothData['text'].apply(lambda tweet: TextBlob(tweet).sentiment.subjectivity)\n",
    "\n",
    "df_C1 = clintonData.groupby('created_str').mean()\n",
    "df_T1 = trumpData.groupby('created_str').mean()\n",
    "df_B1 = bothData.groupby('created_str').mean()\n",
    "\n",
    "print(data.shape)\n",
    "print(clintonData.shape)\n",
    "print(trumpData.shape)\n",
    "print(bothData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dates back into column instead of being an index\n",
    "df_C1.to_csv('csv/clinton_1.csv', sep=',')\n",
    "df_C1 = pd.read_csv('csv/clinton_1.csv')\n",
    "\n",
    "df_T1.to_csv('csv/trump_1.csv', sep=',')\n",
    "df_T1 = pd.read_csv('csv/trump_1.csv')\n",
    "\n",
    "df_B1.to_csv('csv/both_1.csv', sep=',')\n",
    "df_B1 = pd.read_csv('csv/both_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generate Graphs with Pygal before preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_style = Style(colors=('red','blue','purple'))\n",
    "date_chart = pygal.Line(x_label_rotation=60, fill=False, style=custom_style)\n",
    "date_chart.title = 'Polarity Over Time'\n",
    "date_chart.x_labels = df_C1['created_str']\n",
    "date_chart.add(\"trump\", df_T1['polarity'])\n",
    "date_chart.add(\"Clinton\", df_C1['polarity'])\n",
    "date_chart.add(\"Both\", df_B1['polarity'])\n",
    "date_chart.render_to_file('images/polarity_date.svg')\n",
    "\n",
    "date_chart = pygal.Line(x_label_rotation=60, fill=False, style=custom_style)\n",
    "date_chart.title = 'Subjectivity Over Time'\n",
    "date_chart.x_labels = df_C1['created_str']\n",
    "date_chart.add(\"trump\", df_T1['subjectivity'])\n",
    "date_chart.add(\"Clinton\", df_C1['subjectivity'])\n",
    "date_chart.add(\"Both\", df_B1['subjectivity'])\n",
    "date_chart.render_to_file('images/subjectivity_date.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_style = Style(colors=('red',))\n",
    "gauge = pygal.SolidGauge(inner_radius=0.70, style=custom_style, show_legend=False)\n",
    "percent_formatter = lambda x: '{:.10g}%'.format(x)\n",
    "gauge.title = 'trump'\n",
    "gauge.value_formatter = percent_formatter\n",
    "gauge.add('trump', [{'value': round(len(trumpData)/len(data)*100,1), 'max_value': 100}])\n",
    "gauge.render_to_file('images/t_guage.svg')\n",
    "\n",
    "custom_style = Style(colors=('blue',))\n",
    "gauge = pygal.SolidGauge(inner_radius=0.70, style=custom_style, show_legend=False)\n",
    "percent_formatter = lambda x: '{:.10g}%'.format(x)\n",
    "gauge.title = 'Clinton'\n",
    "gauge.value_formatter = percent_formatter\n",
    "gauge.add('Clinton', [{'value': round(len(clintonData)/len(data)*100,1), 'max_value': 100}])\n",
    "gauge.render_to_file('images/c_guage.svg')\n",
    "\n",
    "custom_style = Style(colors=('purple',))\n",
    "gauge = pygal.SolidGauge(inner_radius=0.70, style=custom_style, show_legend=False)\n",
    "percent_formatter = lambda x: '{:.10g}%'.format(x)\n",
    "gauge.title = 'Both'\n",
    "gauge.value_formatter = percent_formatter\n",
    "gauge.add('Both', [{'value': round(len(bothData)/len(data)*100,1), 'max_value': 100}])\n",
    "gauge.render_to_file('images/b_guage.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindex dataframe\n",
    "bothData = bothData.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Example Tweets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  polarity\n",
      "0   RT @luvGodncountry: Wikileaks: Clinton Adviser...      -1.0\n",
      "1   RT @andersonDrLJA: #BillClinton #Obama #Hillar...      -1.0\n",
      "2   RT @venus58: Hey @ShepNewsTeam your show was A...      -1.0\n",
      "3   RT @Col_Connaughton: ASSANGE: CLINTON MEDIA 'E...      -1.0\n",
      "4   Tennessee GOP\\n@TEN_GOP\\nTrump: \"Hillary Clint...      -1.0\n",
      "5   RT @HelloHello228: Terrible I'll bet that hurt...      -1.0\n",
      "6   RT @netwrkguy: @StopStopHillary @sluggoD54 Loo...      -1.0\n",
      "7   ðŸ’£ BREAKING!!!!! ðŸ’£\\n\\nâ€˜Calibration errorâ€™ chang...      -1.0\n",
      "8                  @HillaryClinton VETERANS HATE YOU!      -1.0\n",
      "9   RT @kevkid79: \"Clinton Camp Claims Media Was P...      -1.0\n",
      "10  Bernie campaign director endorses Trump, slams...      -1.0\n",
      "11  RT @Merry__Can: @HillaryClinton #democRATS are...      -1.0\n",
      "12  SHOCKING: Leaked photo of Hillary Clinton with...      -1.0\n",
      "13  Hillary Clinton protects serial rapist Bill Cl...      -1.0\n",
      "14  RT @bob_owens: If Trump is the worst candidate...      -1.0\n",
      "15  RT @randyshort: Moment Donald Trump Won The El...      -1.0\n",
      "16  RT @andersonDrLJA: #BillClinton #Obama #Hillar...      -1.0\n",
      "17  RT @abusedtaxpayer: Hillary's Worst Health Pro...      -1.0\n",
      "18  RT @andersonDrLJA: #BillClinton #Obama #Hillar...      -1.0\n",
      "19  RT @drapermark37: I PISS ON #LIBERALS! THEY WI...      -1.0\n"
     ]
    }
   ],
   "source": [
    "clintonData = clintonData.sort_values(by=['polarity'])\n",
    "clintonData = clintonData.reset_index(drop=True)\n",
    "print(clintonData.loc[0:19,['text','polarity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  subjectivity\n",
      "0   RT @mcicero10: #BernieSanders #Trump people sh...           0.0\n",
      "1   RT @ChristiChat: MT @ChristiChat: #WakeUpAmeri...           0.0\n",
      "2   RT @SCLconservative: The State Controlled Medi...           0.0\n",
      "3   RT @FreedomChild3: Seven Ways Obama Is Trying ...           0.0\n",
      "4   RT @theglobaluniter: @realDonaldTrump \\n\\nðŸ’¥ Wh...           0.0\n",
      "5   RT @Thiru_0914: ðŸ‡ºðŸ‡¸Donald J. Trump Rally SATURD...           0.0\n",
      "6   RT @Trump__Pence: VIDEO : Senator Tim Scott Pr...           0.0\n",
      "7   RT @business: Bloomberg exclusive: Vladimir Pu...           0.0\n",
      "8   RT @realDonaldTrump: Today in Florida, I pledg...           0.0\n",
      "9   Man is trying to scale Trump Tower in NYC usin...           0.0\n",
      "10  RT @GoForTimmer: #ItsUnacceptableTo Act like r...           0.0\n",
      "11  .@ericbolling: \"Can you imagine the president-...           0.0\n",
      "12  RT @TrueCOT: Team Hillary Gave GOP Establishme...           0.0\n",
      "13  Driver uses cardboard Trump head in carpool la...           0.0\n",
      "14  RT @goldengateblond: Dear Media: When Trump sa...           0.0\n",
      "15  RT @blicqer: Media Need 12-Step Program for Do...           0.0\n",
      "16  #TrumpCampaignSlogans Guinea-pig wigs for ever...           0.0\n",
      "17  RT @JDiamond1: A Trump supporter just grabbed ...           0.0\n",
      "18  RT @KlayVolk: Hameed Darweesh The Man Just Rel...           0.0\n",
      "19  RT @CO2HOG: Gingrich: â€˜Without one-sided assau...           0.0\n"
     ]
    }
   ],
   "source": [
    "trumpData = trumpData.sort_values(by=['subjectivity'])\n",
    "trumpData = trumpData.reset_index(drop=True)\n",
    "print(trumpData.loc[0:19,['text','subjectivity']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenize Tweets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clintonData['tokenized_sents'] = clintonData.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "trumpData['tokenized_sents'] = trumpData.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "bothData['tokenized_sents'] = bothData.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lemmatize Tweets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "clintonData['text_lemmatized'] = clintonData.tokenized_sents.apply(lemmatize_text)\n",
    "trumpData['text_lemmatized'] = trumpData.tokenized_sents.apply(lemmatize_text)\n",
    "bothData['text_lemmatized'] = bothData.tokenized_sents.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Remove Stop Words From Tweets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "custom_stopwords = ['RT','https','http','@',':']\n",
    "stop.extend(custom_stopwords)\n",
    "\n",
    "clintonData['without_stopwords'] = clintonData.text_lemmatized.apply(lambda x: [item for item in x if item not in stop])\n",
    "trumpData['without_stopwords'] = trumpData.text_lemmatized.apply(lambda x: [item for item in x if item not in stop])\n",
    "bothData['without_stopwords'] = bothData.text_lemmatized.apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Join tokens back together</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clintonData['processed'] = clintonData.without_stopwords.apply(lambda x: ' '.join(word for word in x))\n",
    "trumpData['processed'] = trumpData.without_stopwords.apply(lambda x: ' '.join(word for word in x))\n",
    "bothData['processed'] = bothData.without_stopwords.apply(lambda x: ' '.join(word for word in x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Processed Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  \\\n",
      "0   RT @NahBabyNah: Twitchy: Chuck Todd caught out...   \n",
      "1   RT @mcicero10: #BernieSanders #Trump people sh...   \n",
      "2   RT @ItsJustJaynie: @HillaryClinton The undecid...   \n",
      "3   @NickTomaWBRE Hi, Nick! We're holding a \"Miner...   \n",
      "4   RT @HillaryClinton: This one's for you, Hillar...   \n",
      "5   RT @leonpui_: Hillary Clinton, Obama and the D...   \n",
      "6   #TrumpBecause #DonaldTrump will not be bought!...   \n",
      "7   RT @PrisonPlanet: Hillary's anti-Trump poster ...   \n",
      "8   RT @TrumpSuperPAC: #AfricanAmericans like @Jer...   \n",
      "9   RT @American_Woman4: #MAGA,#FEMININEAMERICA4TR...   \n",
      "10  RT @Conservatexian: News post: \"TWITTER Buries...   \n",
      "11  RT @1_Hoof_Hearted: @TuckerCarlson\\r\\n@JRubinB...   \n",
      "12  RT @babysgramma: @upayr Obama rules by exec or...   \n",
      "13  Trump appears to encourage gun owners to take ...   \n",
      "14  Obama on Trump winning: 'Anything's possible' ...   \n",
      "15  RT @stormynights10: #TrumpsFavoriteHeadline Tr...   \n",
      "16  RT @Grummz: CNN: \"we got played\"\\nTranslation:...   \n",
      "17  RT @BlastingNews: Trump sued for violating Con...   \n",
      "18  Vice President Joe Biden talks up Hillary Clin...   \n",
      "19  RT @Mr_Nielsen_5309: More proof that @HillaryC...   \n",
      "\n",
      "                                            processed  \n",
      "0   NahBabyNah Twitchy Chuck Todd caught shilling ...  \n",
      "1   mcicero10 # BernieSanders # Trump people rally...  \n",
      "2   ItsJustJaynie HillaryClinton The undecided vot...  \n",
      "3   NickTomaWBRE Hi , Nick ! We 're holding `` Min...  \n",
      "4   HillaryClinton This one 's , Hillary . //t.co/...  \n",
      "5   leonpui_ Hillary Clinton , Obama Democrats use...  \n",
      "6   # TrumpBecause # DonaldTrump bought ! He know ...  \n",
      "7   PrisonPlanet Hillary 's anti-Trump poster chil...  \n",
      "8   TrumpSuperPAC # AfricanAmericans like JermonMa...  \n",
      "9   American_Woman4 # MAGA , # FEMININEAMERICA4TRU...  \n",
      "10  Conservatexian News post `` TWITTER Buries 32 ...  \n",
      "11  1_Hoof_Hearted TuckerCarlson JRubinBlogger # B...  \n",
      "12  babysgramma upayr Obama rule exec order , wish...  \n",
      "13  Trump appears encourage gun owner take action ...  \n",
      "14  Obama Trump winning 'Anything 's possible ' # ...  \n",
      "15  stormynights10 # TrumpsFavoriteHeadline Trump ...  \n",
      "16  Grummz CNN `` got played '' Translation Trump ...  \n",
      "17  BlastingNews Trump sued violating Constitution...  \n",
      "18  Vice President Joe Biden talk Hillary Clinton ...  \n",
      "19  Mr_Nielsen_5309 More proof HillaryClinton & am...  \n"
     ]
    }
   ],
   "source": [
    "print(bothData.loc[0:19,['text','processed']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Perform sentiment analysis on data after preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add polarity and subjectivity columns to dataframes\n",
    "clintonData['polarity_2'] = clintonData['processed'].apply(lambda tweet: TextBlob(tweet).sentiment.polarity)\n",
    "clintonData['subjectivity_2'] = clintonData['processed'].apply(lambda tweet: TextBlob(tweet).sentiment.subjectivity)\n",
    "trumpData['polarity_2'] = trumpData['processed'].apply(lambda tweet: TextBlob(tweet).sentiment.polarity)\n",
    "trumpData['subjectivity_2'] = trumpData['processed'].apply(lambda tweet: TextBlob(tweet).sentiment.subjectivity)\n",
    "bothData['polarity_2'] = bothData['processed'].apply(lambda tweet: TextBlob(tweet).sentiment.polarity)\n",
    "bothData['subjectivity_2'] = bothData['processed'].apply(lambda tweet: TextBlob(tweet).sentiment.subjectivity)\n",
    "\n",
    "\n",
    "df_C2 = clintonData.groupby('created_str').mean()\n",
    "df_T2 = trumpData.groupby('created_str').mean()\n",
    "df_B2 = bothData.groupby('created_str').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dates back into column instead of being an index\n",
    "df_C2.to_csv('csv/clinton_2.csv', sep=',')\n",
    "df_C2 = pd.read_csv('csv/clinton_2.csv')\n",
    "\n",
    "df_T2.to_csv('csv/trump_2.csv', sep=',')\n",
    "df_T2 = pd.read_csv('csv/trump_2.csv')\n",
    "\n",
    "df_B2.to_csv('csv/both_2.csv', sep=',')\n",
    "df_B2 = pd.read_csv('csv/both_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generate graphs with Pygal after preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_style = Style(colors=('red','blue','purple'))\n",
    "date_chart = pygal.Line(x_label_rotation=60, fill=False, style=custom_style)\n",
    "date_chart.title = 'Polarity Over Time'\n",
    "date_chart.x_labels = df_C2['created_str']\n",
    "date_chart.add(\"trump\", df_T2['polarity_2'])\n",
    "date_chart.add(\"Clinton\", df_C2['polarity_2'])\n",
    "date_chart.add(\"Both\", df_B2['polarity_2'])\n",
    "date_chart.render_to_file('images/polarity_date_processed.svg')\n",
    "\n",
    "date_chart = pygal.Line(x_label_rotation=60, fill=False, style=custom_style)\n",
    "date_chart.title = 'Subjectivity Over Time'\n",
    "date_chart.x_labels = df_C2['created_str']\n",
    "date_chart.add(\"trump\", df_T2['subjectivity_2'])\n",
    "date_chart.add(\"Clinton\", df_C2['subjectivity_2'])\n",
    "date_chart.add(\"Both\", df_B2['subjectivity_2'])\n",
    "date_chart.render_to_file('images/subjectivity_date_processed.svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
